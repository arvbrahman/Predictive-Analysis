#KNN on prostate cancer----
prostate_data <- read.csv(file = "Prostate_cancer.csv",stringsAsFactors = F)
setwd("~/GitHub/Predictive-Analysis/KNN")
#KNN on prostate cancer----
prostate_data <- read.csv(file = "Prostate_cancer.csv",stringsAsFactors = F)
prostate_data <- prostate_data[,-1]
#Function for normalization
norm <- function(x){
(x-min(x))/(max(x)-min(x))
}
prostate_norm <- as.data.frame(lapply(prostate_data[,-1],norm))  #standardizing
#Splitting the Data into train and test
prostate_train <- prostate_norm[1:70,]
prostate_test <- prostate_norm[71:100,]
#KNN
library(class)  #class library contains knn
prostate_pred <- knn(prostate_train,prostate_test,prostate_data[1:70,1],k=6)
table(prostate_pred,prostate_data[71:100,1])
#Evaluation
library(gmodels) #gmodels library contains crosstable
CrossTable(prostate_data[71:100,1],prostate_pred,prop.chisq = F)
setwd("~/GitHub/Predictive-Analysis/Naive Bayes")
setwd("~/GitHub/Predictive-Analysis/Naive Bayes")
#Loading necessary libraries ----
library(dplyr)   #for transforming data and contains pipe operator
library(tm)   #tm: text mining library is used for text cleaning
library(SnowballC)   #Library for stemming
library(caret)    #Library for splitting data
library(wordcloud)   #library for pictorial representation of frequency
library(e1071)   #package for NaiveBayes
library(gmodels)  #library for crosstable for confusionmatrix
#loading data ----
sms_raw <- read.csv(file = "spam.csv", stringsAsFactors = FALSE)
#Cleaning  ----
sms_raw$type <- factor(sms_raw$type)
sms_raw <- sms_raw[,c("type","text")]
#loading data ----
sms_raw <- read.csv(file = "spam.csv", stringsAsFactors = FALSE)
#Cleaning  ----
sms_raw$type <- factor(sms_raw$type)
sms_raw <- sms_raw[,c("type","text")]
#Text Cleaning: ----
sms_corpus<-iconv(sms_raw$text)
sms_corpus_clean <- Corpus(VectorSource(sms_corpus))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, stopwords())%>%
tm_map(removePunctuation)%>%
tm_map(stemDocument)%>%
tm_map(stripWhitespace)
#Document-Term Matrix (DTM) & splitting ----
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#Document-Term Matrix (DTM) & splitting ----
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#Word Cloud ----
wordcloud(sms_corpus_clean, min.freq = 50,
random.order = FALSE,colors=brewer.pal(5,"Dark2"))
#findFreqTerms
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
#convert_counts ----
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#Model training ----
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
#CrossTable ----
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
confusionMatrix(sms_test_pred,sms_test_labels)
confusionMatrix(sms_test_pred,sms_test_labels)
#CrossTable ----
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
confusionMatrix(sms_test_pred,sms_test_labels)
#Loading necessary libraries ----
library(dplyr)   #for transforming data and contains pipe operator
library(tm)   #tm: text mining library is used for text cleaning
library(SnowballC)   #Library for stemming
library(caret)    #Library for splitting data
library(wordcloud)   #library for pictorial representation of frequency
library(e1071)   #package for NaiveBayes
library(gmodels)  #library for crosstable for confusionmatrix
#Loading the data ----
mail_Data <- read.csv(file = "spam.csv",stringsAsFactors = F)
#Observing the data ----
str(mail_Data)
table(mail_Data$type)
#cleaning and transforming the data ----
#Removing unnecessary columns &
#converting label into categorical variable which makes it easy for classification
mail_Data <- mail_Data[,c("type","text")]
mail_Data$type <- as.factor(mail_Data$type)
#Data Preprocessing ----
#text cleaning in this data set
mail_corpus <- iconv(mail_Data$text)
mail_corpus_clean<-  Corpus(VectorSource(mail_corpus))%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
tm_map(removePunctuation)%>%
tm_map(removeWords,stopwords())%>%
tm_map(stripWhitespace)%>%
tm_map(stemDocument)
#DTM
mail_dtm <- DocumentTermMatrix(mail_corpus_clean)
#Splitting the data ----
set.seed(123)
trainIndex <- createDataPartition(mail_Data$type, p = 0.8, list = F)
mail_train_data <- mail_dtm[trainIndex, ]
mail_test_data <- mail_dtm[-trainIndex, ]
mail_train_labels <- mail_Data$type[trainIndex]
mail_test_labels <- mail_Data$type[-trainIndex]
#Wordcloud ----
wordcloud(mail_corpus_clean,min.freq = 50,
random.order = F,colors = brewer.pal(5,"Dark2"))
#Frequency
findFreqTerms(mail_train_data,5)
mail_freq_words <- findFreqTerms(mail_train_data, 5)
str(mail_freq_words)
mail_freq_train<- mail_train_data[ , mail_freq_words]
mail_freq_test <- mail_test_data[ , mail_freq_words]
#Training the model ----
model <- naiveBayes(as.matrix(mail_freq_train),mail_train_labels)
predictions <- predict(model,as.matrix(mail_test_data))
#Evaluating ----
confusionMatrix(predictions,mail_test_labels)
CrossTable(predictions,mail_test_labels,prop.chisq = F)
setwd("~/GitHub/Predictive-Analysis/Decision Trees")
#Libraries
library(rpart)
library(rpart.plot)
#Analyze
summary(iris)
str(iris)
#Splitting
indexes = sample(150,110)
iris.train <- iris[indexes,]
iris.train
iris.test<- iris[-indexes,]
iris.test
#Separate target variable(Dependent variable)
target = Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
target
#Train model
tree = rpart(target, data = iris.train, method = "class")
tree
#Plot the tree used
rpart.plot(tree)
#Check predictions on test data
predict(tree,iris.test)
source("~/GitHub/Predictive-Analysis/Decision Trees/Dtree_on_iris.R")
#Required Libraries ----
library(ISLR)     #for Carseats data
library(rpart)
library(tree)
library(rpart.plot)
#Loading and splitting the data ----
data(Carseats)
Carseats$Wsales <- as.factor(ifelse(Carseats$Sales>=7,"Yes","No"))
index <- sample(400,350)
Cs.train <- Carseats[index,]
Cs.test <- Carseats[-index,]
#Separating the target variable ----
target <- Wsales ~ . -Sales
Model <- rpart(target,Cs.train,method = "class")
Tree <- tree(target,Cs.train,mindev = 0.01)
#Plotting the tree
rpart.plot(Model)
plot(Tree)
text(Tree,pretty = 0)
#Predictions
predictions <- predict(Model,Cs.test,type = "class")
predictions
p<- predict(Tree,Cs.test,type = "class")
p
#Pruning ----
set.seed(2)
pruned <- cv.tree(Tree, FUN = prune.misclass)
plot(pruned$size,pruned$dev,type = "b",xlab = "Tree size",ylab = "Deviance")
pruned.tree <- prune.misclass(Tree,best = 9)
plot(pruned.tree)
text(pruned.tree,pretty = 0)
#Libraries
library(rpart)
library(rpart.plot)
#Analyze
summary(iris)
str(iris)
#Splitting
indexes = sample(150,110)
iris.train <- iris[indexes,]
iris.train
iris.test<- iris[-indexes,]
iris.test
#Libraries
library(rpart)
library(rpart.plot)
#Analyze
summary(iris)
str(iris)
#Splitting
indexes = sample(150,110)
iris.train <- iris[indexes,]
iris.train
iris.test<- iris[-indexes,]
iris.test
#Separate target variable(Dependent variable)
target = Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
target
#Train model
tree = rpart(target, data = iris.train, method = "class")
tree
#Plot the tree used
rpart.plot(tree)
#Check predictions on test data
predict(tree,iris.test)
source("~/GitHub/Predictive-Analysis/Decision Trees/Dtree_on_iris.R")
#Required Libraries ----
library(ISLR)     #for Carseats data
library(rpart)
library(tree)
library(rpart.plot)
#Loading and splitting the data ----
data(Carseats)
Carseats$Wsales <- as.factor(ifelse(Carseats$Sales>=7,"Yes","No"))
index <- sample(400,350)
Cs.train <- Carseats[index,]
Cs.test <- Carseats[-index,]
#Separating the target variable ----
target <- Wsales ~ . -Sales
Model <- rpart(target,Cs.train,method = "class")
Tree <- tree(target,Cs.train,mindev = 0.01)
#Plotting the tree
rpart.plot(Model)
plot(Tree)
text(Tree,pretty = 0)
#Predictions
predictions <- predict(Model,Cs.test,type = "class")
predictions
p<- predict(Tree,Cs.test,type = "class")
p
#Pruning ----
set.seed(2)
pruned <- cv.tree(Tree, FUN = prune.misclass)
plot(pruned$size,pruned$dev,type = "b",xlab = "Tree size",ylab = "Deviance")
pruned.tree <- prune.misclass(Tree,best = 9)
plot(pruned.tree)
text(pruned.tree,pretty = 0)
#Pruning ----
set.seed(2)
pruned <- cv.tree(Tree, FUN = prune.misclass)
plot(pruned$size,pruned$dev,type = "b",xlab = "Tree size",ylab = "Deviance")
pruned.tree <- prune.misclass(Tree,best = 9)
plot(pruned.tree)
text(pruned.tree,pretty = 0)
pruned <- cv.tree(Tree, FUN = prune.misclass)
plot(pruned$size,pruned$dev,type = "b",xlab = "Tree size",ylab = "Deviance")
#Required libraries
library(caTools)   #for splitting the data
library(rpart)     #for decision tree
library(rpart.plot)
library(ggplot2)
#Loading the data
Ps <- read.csv(file = "Position_Salaries.csv",stringsAsFactors = F)
#Selecting the fields required
Ps <- Ps[,2:3]
#Splitting the data into train and test
set.seed(3)
split <- sample.split(Ps$Salary,SplitRatio = 0.65)
train.set <- subset(Ps,split==T)
test.set <- subset(Ps,split==F)
model <- rpart(Salary ~ .,
train.set,
control = rpart.control(minsplit = 2, cp = 0.01))
model <- rpart(Salary ~ .,
train.set,
control = rpart.control(minsplit = 2, cp = 0.01))
#Required libraries
library(caTools)   #for splitting the data
library(rpart)     #for decision tree
library(rpart.plot)
library(ggplot2)
#Loading the data
Ps <- read.csv(file = "Position_Salaries.csv",stringsAsFactors = F)
#Selecting the fields required
Ps <- Ps[,2:3]
#Splitting the data into train and test
set.seed(3)
split <- sample.split(Ps$Salary,SplitRatio = 0.65)
train.set <- subset(Ps,split==T)
test.set <- subset(Ps,split==F)
#Model
model <- rpart(Salary ~ .,
train.set,
control = rpart.control(minsplit = 2, cp = 0.01))
model
#Decision Tree
rpart.plot(model)
#
x_grid <- seq(min(Ps$Level),max(Ps$Level), 0.01)
ggplot(Ps,aes(Level,Salary))+
geom_point()+
geom_line()
model
#
h<- c(6.1,9.1,6.9,5.6,6.7,8.9,7.8,6.8,7.9,6.6)
w<- c(55,56,67,69,78,89,78,56,56,60)
#
relation<- lm(w~h)
relation
df <- data.frame(h=c(5.6,6.2,6.4,5.3))
predict(relation,df)
#Q1 ----
#Create a dataframe
house <- data.frame(size=c(1500,1800,2000,2200,2500,3000,3500,4000,4500,5000),
bedrooms=c(3,3,4,4,5,5,5,6,6,6),
bathrooms=c(2,2,3,3,4,4,3,5,4,5),
age_years=c(10,5,8,12,6,3,15,7,4,2),
price=c(300000,450000,350000,420000,500000,600000,580000,600000,750000,800000))
#Linear Regression model
model<- lm(price ~ .,house)
model
#Predicting
df<- data.frame(size=3200,bedrooms=5,bathrooms=4,age_years=8)
predicted<- predict(model,df)
cat("Predicted selling price: ",predicted,"/n")
#Q2 ----
#Creating a dataframe
emp <-  data.frame(exp=c(1,2,3,4,5,6,7,8,9,10),
edu=c(12,14,16,12,16,14,12,14,16,12),
age=c(21,23,24,25,26,23,27,28,27,25),
salary=c(50,55,60,62,58,65,68,70,72,75))
#Model (Linear Regression)
model<- lm(salary ~ .,emp)
model
#Prediction
df <- data.frame(exp=4,edu=16,age=24)
predicted<- predict(model,df)
cat("Predicted salary for the employee: $",predicted,"\n")
#Packages Required
library(ggplot2)
#loading the dataset
possal <- read.csv(file = "Position_Salaries.csv",stringsAsFactors = F)
#Analyzing
str(possal)
summary(possal)
table(possal$Position)
hist(possal$Level)
#Selecting 2nd and 3rd column
pos <- possal[2:3]
#Linear Regression Model
lm.model <- lm(Salary ~ .,pos)
summary(lm.model)
#Fitting some variables for Polynomial Regression
pos$Level2 <- pos$Level^2
pos$Level3 <- pos$Level^3
pos$Level4 <- pos$Level^4
#Polynomial Regression Model
poly.model <- lm(Salary ~ . ,pos)
summary(poly.model)
#Visualizing both the models
ggplot()+
geom_point(aes(Level,Salary),pos,colour = "green")+
geom_line(aes(Level,predict(lm.model,pos)),pos,colour = "orange")+
ggtitle("Truth or Bluff(Linear Regression)")+
theme_dark()
ggplot()+
geom_point(aes(Level,Salary),pos,colour = "green")+
geom_line(aes(Level,predict(poly.model,pos)),pos,colour = "orange")+
ggtitle("Truth or Bluff(Polynomial Regression)")+
theme_dark()
#Visualizing the Regression Model results (for higher resolution and smoother curve)
x_grid <- seq(min(pos$Level),max(pos$Level),0.1)
ggplot()+
geom_point(aes(Level,Salary),
pos,colour = "blue")+
geom_line(aes(x_grid,predict(poly.model,data.frame(Level = x_grid,
Level2 = x_grid^2,
Level3 = x_grid^3,
Level4 = x_grid^4))),
colour = "red")+
ggtitle("Polynomial Regression")+
theme_dark()
#Libraries ----
library(psych)
#Loading the dataset ----
ins <-  read.csv(file = "insurance.csv",stringsAsFactors = F)
ins$age2 <- ins$age^2
ins$bmi30 <- ifelse(ins$bmi>=30,1,0)
#Modelling ----
model <- lm(charges ~ . ,ins)
#Analyzing ----
hist(ins$age)
table(ins$region)
cor(ins[c("age","bmi","children","charges")])
describe(ins)
pairs(ins[c("age","bmi","children","charges")])
pairs.panels(ins[c("age","bmi","children","charges")])
cor(ins[c("age","bmi","children","charges")])
#Prediction ----
df2<- data.frame(age=25,sex="female",bmi=30.2,children=2,smoker="no",region="northeast",age2=625,bmi30=1)
predict(model,df2)
#Analyzing ----
hist(ins$age)
table(ins$region)
cor(ins[c("age","bmi","children","charges")])
describe(ins)
pairs(ins[c("age","bmi","children","charges")])
pairs.panels(ins[c("age","bmi","children","charges")])
#Packages Required
library(ggplot2)
#loading the dataset
possal <- read.csv(file = "Position_Salaries.csv",stringsAsFactors = F)
#Analyzing
str(possal)
summary(possal)
table(possal$Position)
hist(possal$Level)
View(possal)
#Selecting 2nd and 3rd column
pos <- possal[2:3]
#Linear Regression Model
lm.model <- lm(Salary ~ .,pos)
summary(lm.model)
#Fitting some variables for Polynomial Regression
pos$Level2 <- pos$Level^2
pos$Level3 <- pos$Level^3
pos$Level4 <- pos$Level^4
#Polynomial Regression Model
poly.model <- lm(Salary ~ . ,pos)
summary(poly.model)
#Visualizing both the models
ggplot()+
geom_point(aes(Level,Salary),pos,colour = "green")+
geom_line(aes(Level,predict(lm.model,pos)),pos,colour = "orange")+
ggtitle("Truth or Bluff(Linear Regression)")+
theme_dark()
ggplot()+
geom_point(aes(Level,Salary),pos,colour = "green")+
geom_line(aes(Level,predict(poly.model,pos)),pos,colour = "orange")+
ggtitle("Truth or Bluff(Polynomial Regression)")+
theme_dark()
seq(ff)#Visualizing the Regression Model results (for higher resolution and smoother curve)
setwd("~/GitHub/Predictive-Analysis/Neural Network")
#Libraries Required
install.packages("neuralnet")
library(neuralnet)
#Dataset
Cd <- read.csv(file = "Concrete_Data.csv",stringsAsFactors = F)
#Analyzing
hist(Cd$strength)
summary(Cd)
str(Cd)
#Normalizing the data
normalize <- function(x){return((x-min(x))/(max(x)-min((x))))}
Cd.norm <- data.frame(lapply(Cd,normalize))
hist(Cd.norm$strength)
#Splitting the data into train and test
Cd.train <- Cd.norm[1:774,]
Cd.test <- Cd.norm[775:1030,]
#Model training
Cd.model <- neuralnet(strength ~ . ,Cd.train,hidden = 7)     #Single Hidden Node
plot(Cd.model)   #plotting
#Libraries Required
library(neuralnet)
#Dataset
Cd <- read.csv(file = "Concrete_Data.csv",stringsAsFactors = F)
#Analyzing
hist(Cd$strength)
summary(Cd)
str(Cd)
#Normalizing the data
normalize <- function(x){return((x-min(x))/(max(x)-min((x))))}
Cd.norm <- data.frame(lapply(Cd,normalize))
hist(Cd.norm$strength)
#Splitting the data into train and test
Cd.train <- Cd.norm[1:774,]
Cd.test <- Cd.norm[775:1030,]
#Model training
Cd.model <- neuralnet(strength ~ . ,Cd.train,hidden = 7)     #Single Hidden Node
plot(Cd.model)   #plotting
#Testing
compute(Cd.model,Cd.test[,1:8])
predict(Cd.model,Cd.test[,1:8])
