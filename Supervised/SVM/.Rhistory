setwd("~/GitHub/Predictive-Analysis/Supervised/SVM")
#Library
library(caTools)    #for splitting the data
library(e1071)      #contains both svm and naiveBayes
#working on iris in-built dataset
i<- iris
summary(i)
hist(i$Sepal.Width)
str(i)
#Splitting into train and test
set.seed(134)
split<- sample.split(i$Species,SplitRatio = 0.75)
i.train <- subset(i,split==T)
i.test<- subset(i,split==F)
#Library
library(caTools)    #for splitting the data
library(e1071)      #contains svm function
#working on chickweight in-built dataset
data<- ChickWeight
summary(data)
hist(data$weight)
str(data)
#Splitting into train and test
set.seed(134)
split<- sample.split(data$weight,SplitRatio = 0.7)
train_x <- subset(data,split==T)
test_y <- subset(data,split==F)
help("svm")
#Training the model
Model <- svm(Weight ~ . ,
train_x,
type = 'nu-regression',
kernel = 'linear')
View(data)
#Training the model
Model <- svm(weight ~ . ,
train_x,
type = 'nu-regression',
kernel = 'linear')
Model
#Prediction
predict(Model,test_y)
View(data)
View(data)
#Training the model
Model <- svm(weight ~ . -Chick ,
train_x,
type = 'nu-regression',
kernel = 'linear')
Model
#Prediction
predict(Model,test_y)
#Libraries Required
library(neuralnet)
#Dataset
Cd <- read.csv(file = "Concrete_Data.csv",stringsAsFactors = F)
#Analyzing
hist(Cd$strength)
summary(Cd)
str(Cd)
#Normalizing the data
normalize <- function(x){return((x-min(x))/(max(x)-min((x))))}
Cd.norm <- data.frame(lapply(Cd,normalize))
hist(Cd.norm$strength)
#Splitting the data into train and test
Cd.train <- Cd.norm[1:774,]
Cd.test <- Cd.norm[775:1030,]
#Library
library(caTools)    #for splitting the data
library(e1071)      #contains both svm and naiveBayes
#working on iris in-built dataset
i<- iris
summary(i)
hist(i$Sepal.Width)
str(i)
#Splitting into train and test
set.seed(134)
split<- sample.split(i$Species,SplitRatio = 0.75)
i.train <- subset(i,split==T)
i.test<- subset(i,split==F)
#Required libraries
library(caTools)   #for splitting the data
library(rpart)     #for decision tree
library(rpart.plot)
library(ggplot2)
#Loading the data
data <- read.csv(file = "Cardiotocographic.csv",stringsAsFactors = F)
setwd("~/GitHub/Predictive-Analysis/Supervised/Decision Trees")
#Loading the data
data <- read.csv(file = "Cardiotocographic.csv",stringsAsFactors = F)
View(data)
#Required Libraries
library(kernlab)
library(caTools)     #For splitting the data
#Dataset being used is the letterrecognition data
letter_data <- read.csv(file = "letterrecognition.csv",stringsAsFactors = T)
setwd("~/GitHub/Predictive-Analysis/Supervised/SVM")
#Dataset being used is the letterrecognition data
letter_data <- read.csv(file = "letterrecognition.csv",stringsAsFactors = T)
str(letter_data)
summary(letter_data)
#splitting the data
set.seed(123)
spl <- sample.split(letter_data$letter,SplitRatio = 0.8)
Train.letter <- subset(letter_data,spl==T)
Test.letter <- subset(letter_data,spl==F)
#Training the model with vanilladot
Model<- ksvm(letter ~ .,
Train.letter,
kernel = "vanilladot")
Model
#Predicting on the testing data
Predictions<- predict(Model,Test.letter)
table(Predictions, Test.letter$letter)
agreement <- Predictions==Test.letter$letter
table(agreement)
prop.table(table(agreement))
#Training with rbfdot kernel
Model.rbf<- ksvm(letter ~ .,
Train.letter,
kernel = "rbfdot")    #Radial Basic Function, when linear cannot be used on the data
Model.rbf
#Predicting
predict.rbf <- predict(Model.rbf,Test.letter)
table(predict.rbf,Test.letter$letter)
agreement.rbf <- predict.rbf==Test.letter$letter
table(agreement.rbf)
prop.table(table(agreement.rbf))
